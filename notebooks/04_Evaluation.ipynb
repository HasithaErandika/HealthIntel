{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-04T14:14:53.200349Z",
     "start_time": "2025-08-04T14:14:36.297726Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from joblib import load\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --------------------- Define Paths ---------------------\n",
    "processed_data_path = '../data/processed/'\n",
    "models_path = '../models/'\n",
    "\n",
    "# --------------------- Load Test Data ---------------------\n",
    "X_test = np.load(os.path.join(processed_data_path, 'X_test.npy'))\n",
    "y_test = np.load(os.path.join(processed_data_path, 'y_test.npy'))\n",
    "\n",
    "# --------------------- Load Models ---------------------\n",
    "log_reg = load(os.path.join(models_path, 'logistic_regression_smote.pkl'))\n",
    "rf_model = load(os.path.join(models_path, 'random_forest_smote_tuned.pkl'))\n",
    "xgb_model = load(os.path.join(models_path, 'xgboost_tuned.pkl'))\n",
    "nn_model = load_model(os.path.join(models_path, 'neural_net_tuned.h5'))\n",
    "\n",
    "# --------------------- Evaluation Function ---------------------\n",
    "def evaluate_model(model, model_name, X_test, y_test):\n",
    "    if model_name == 'Neural Network':\n",
    "        y_pred_proba = model.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "    # Classification Report\n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # ROC-AUC Score\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "    print(f\"ROC-AUC Score for {model_name}: {roc_auc:.4f}\")\n",
    "    return classification_report(y_test, y_pred), roc_auc\n",
    "\n",
    "# --------------------- Evaluate and Save Results ---------------------\n",
    "results_file = os.path.join(models_path, 'evaluation_results.txt')\n",
    "with open(results_file, 'w') as f:\n",
    "    models = {\n",
    "        'Logistic Regression': log_reg,\n",
    "        'Random Forest': rf_model,\n",
    "        'XGBoost': xgb_model,\n",
    "        'Neural Network': nn_model\n",
    "    }\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if name == 'Neural Network':\n",
    "            y_pred_proba = model.predict(X_test)\n",
    "            y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "\n",
    "        print(f\"\\n{name} evaluated. Saving results...\")\n",
    "        f.write(f\"\\nClassification Report for {name}:\\n{class_report}\\n\")\n",
    "        f.write(f\"ROC-AUC Score: {roc_auc:.4f}\\n\")\n",
    "\n",
    "print(\"\\nEvaluation complete. Results saved to evaluation_results.txt.\")\n",
    "\n",
    "# --------------------- Use Neural Network for Final Predictions ---------------------\n",
    "y_pred_proba = nn_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Map numeric labels to human-readable labels\n",
    "label_map = {\n",
    "    0: 'No Diabetes',\n",
    "    1: 'Yes, no complications',\n",
    "    2: 'Yes, with complications'\n",
    "}\n",
    "y_pred_labels = [label_map[pred] for pred in y_pred]\n",
    "y_test_labels = [label_map[true] for true in y_test]\n",
    "\n",
    "# Create a DataFrame with predictions and confidence scores\n",
    "results_df = pd.DataFrame({\n",
    "    'True Label': y_test_labels,\n",
    "    'Predicted Label': y_pred_labels,\n",
    "    'Confidence Score': [y_pred_proba[i, pred] for i, pred in enumerate(y_pred)]\n",
    "})\n",
    "\n",
    "# Display first 10 rows\n",
    "print(\"\\nQuick Outcome Summary (First 10 Predictions):\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "# Save results to CSV\n",
    "output_csv = os.path.join(models_path, 'predictions_outcome.csv')\n",
    "results_df.to_csv(output_csv, index=False)\n",
    "print(f\"\\nFull prediction results saved to {output_csv}\")\n",
    "\n",
    "# --------------------- Optional: Basic Performance Summary ---------------------\n",
    "print(\"\\nPerformance Summary for Neural Network:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted'):.4f}\")\n",
    "\n",
    "print(\"\\nOutcome generated. You can use predictions_outcome.csv for analysis or reporting.\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression evaluated. Saving results...\n",
      "\n",
      "Random Forest evaluated. Saving results...\n",
      "\n",
      "XGBoost evaluated. Saving results...\n",
      "\u001B[1m2453/2453\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 892us/step\n",
      "\n",
      "Neural Network evaluated. Saving results...\n",
      "\n",
      "Evaluation complete. Results saved to evaluation_results.txt.\n",
      "\u001B[1m2453/2453\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 556us/step\n",
      "\n",
      "Quick Outcome Summary (First 10 Predictions):\n",
      "                True Label          Predicted Label  Confidence Score\n",
      "0  Yes, with complications  Yes, with complications          0.596485\n",
      "1              No Diabetes  Yes, with complications          0.511028\n",
      "2              No Diabetes  Yes, with complications          0.653673\n",
      "3              No Diabetes              No Diabetes          0.554781\n",
      "4              No Diabetes              No Diabetes          0.419042\n",
      "5              No Diabetes  Yes, with complications          0.657048\n",
      "6  Yes, with complications  Yes, with complications          0.463760\n",
      "7              No Diabetes  Yes, with complications          0.399435\n",
      "8  Yes, with complications  Yes, with complications          0.806485\n",
      "9              No Diabetes    Yes, no complications          0.373508\n",
      "\n",
      "Full prediction results saved to ../models/predictions_outcome.csv\n",
      "\n",
      "Performance Summary for Neural Network:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.58      0.72     65029\n",
      "         1.0       0.08      0.00      0.00      1974\n",
      "         2.0       0.26      0.86      0.40     11474\n",
      "\n",
      "    accuracy                           0.61     78477\n",
      "   macro avg       0.43      0.48      0.37     78477\n",
      "weighted avg       0.83      0.61      0.66     78477\n",
      "\n",
      "ROC-AUC Score: 0.7909\n",
      "\n",
      "Outcome generated. You can use predictions_outcome.csv for analysis or reporting.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T14:22:47.414343Z",
     "start_time": "2025-08-04T14:22:46.426187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------- Import Libraries ---------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from joblib import load\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --------------------- Define Paths ---------------------\n",
    "processed_data_path = '../data/processed/'\n",
    "models_path = '../models/'\n",
    "\n",
    "# --------------------- Load Balanced Test Data ---------------------\n",
    "X_test_balanced = np.load(os.path.join(processed_data_path, 'X_test_balanced.npy'))\n",
    "y_test_balanced = np.load(os.path.join(processed_data_path, 'y_test_balanced.npy'))\n",
    "\n",
    "# --------------------- Load Balanced Models ---------------------\n",
    "log_reg_balanced = load(os.path.join(models_path, 'logistic_regression_balanced.pkl'))\n",
    "rf_model_balanced = load(os.path.join(models_path, 'random_forest_balanced_tuned.pkl'))\n",
    "xgb_model_balanced = load(os.path.join(models_path, 'xgboost_balanced_tuned.pkl'))\n",
    "nn_model_balanced = load_model(os.path.join(models_path, 'neural_net_balanced_tuned.h5'))\n",
    "\n",
    "# --------------------- Evaluation Function ---------------------\n",
    "def evaluate_model_balanced(model, model_name, X_test_balanced, y_test_balanced):\n",
    "    if model_name == 'Neural Network':\n",
    "        y_pred_proba_balanced = model.predict(X_test_balanced)\n",
    "        y_pred_balanced = np.argmax(y_pred_proba_balanced, axis=1)\n",
    "    else:\n",
    "        y_pred_balanced = model.predict(X_test_balanced)\n",
    "        y_pred_proba_balanced = model.predict_proba(X_test_balanced)\n",
    "\n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    print(classification_report(y_test_balanced, y_pred_balanced))\n",
    "\n",
    "    roc_auc_balanced = roc_auc_score(y_test_balanced, y_pred_proba_balanced, multi_class='ovr', average='weighted')\n",
    "    print(f\"ROC-AUC Score for {model_name}: {roc_auc_balanced:.4f}\")\n",
    "    return classification_report(y_test_balanced, y_pred_balanced), roc_auc_balanced\n",
    "\n",
    "# --------------------- Evaluate and Save Results ---------------------\n",
    "results_file_balanced = os.path.join(models_path, 'evaluation_results_balanced.txt')\n",
    "with open(results_file_balanced, 'w') as f:\n",
    "    models_balanced = {\n",
    "        'Logistic Regression': log_reg_balanced,\n",
    "        'Random Forest': rf_model_balanced,\n",
    "        'XGBoost': xgb_model_balanced,\n",
    "        'Neural Network': nn_model_balanced\n",
    "    }\n",
    "\n",
    "    for name, model in models_balanced.items():\n",
    "        if name == 'Neural Network':\n",
    "            y_pred_proba_balanced = model.predict(X_test_balanced)\n",
    "            y_pred_balanced = np.argmax(y_pred_proba_balanced, axis=1)\n",
    "        else:\n",
    "            y_pred_balanced = model.predict(X_test_balanced)\n",
    "            y_pred_proba_balanced = model.predict_proba(X_test_balanced)\n",
    "\n",
    "        class_report_balanced = classification_report(y_test_balanced, y_pred_balanced)\n",
    "        roc_auc_balanced = roc_auc_score(y_test_balanced, y_pred_proba_balanced, multi_class='ovr', average='weighted')\n",
    "\n",
    "        print(f\"\\n{name} evaluated. Saving results...\")\n",
    "        f.write(f\"\\nClassification Report for {name}:\\n{class_report_balanced}\\n\")\n",
    "        f.write(f\"ROC-AUC Score: {roc_auc_balanced:.4f}\\n\")\n",
    "\n",
    "print(\"\\nBalanced evaluation complete. Results saved to evaluation_results_balanced.txt.\")\n",
    "\n",
    "# --------------------- Neural Network Predictions & Outcome Summary ---------------------\n",
    "y_pred_proba_balanced = nn_model_balanced.predict(X_test_balanced)\n",
    "y_pred_balanced = np.argmax(y_pred_proba_balanced, axis=1)\n",
    "\n",
    "# Map numeric labels to human-readable labels\n",
    "label_map_balanced = {\n",
    "    0: 'No Diabetes',\n",
    "    1: 'Yes, no complications',\n",
    "    2: 'Yes, with complications'\n",
    "}\n",
    "y_pred_labels_balanced = [label_map_balanced[pred] for pred in y_pred_balanced]\n",
    "y_test_labels_balanced = [label_map_balanced[true] for true in y_test_balanced]\n",
    "\n",
    "# Create DataFrame\n",
    "results_df_balanced = pd.DataFrame({\n",
    "    'True Label': y_test_labels_balanced,\n",
    "    'Predicted Label': y_pred_labels_balanced,\n",
    "    'Confidence Score': [y_pred_proba_balanced[i, pred] for i, pred in enumerate(y_pred_balanced)]\n",
    "})\n",
    "\n",
    "# Show quick summary\n",
    "print(\"\\nQuick Outcome Summary (First 10 Predictions):\")\n",
    "print(results_df_balanced.head(10))\n",
    "\n",
    "# Save predictions\n",
    "output_csv_balanced = os.path.join(models_path, 'predictions_outcome_balanced.csv')\n",
    "results_df_balanced.to_csv(output_csv_balanced, index=False)\n",
    "print(f\"\\nFull prediction results saved to {output_csv_balanced}\")\n",
    "\n",
    "# --------------------- Performance Summary (Neural Network) ---------------------\n",
    "print(\"\\nPerformance Summary for Neural Network (Balanced):\")\n",
    "print(classification_report(y_test_balanced, y_pred_balanced))\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test_balanced, y_pred_proba_balanced, multi_class='ovr', average='weighted'):.4f}\")\n",
    "\n",
    "print(\"\\nBalanced outcome generated. You can use predictions_outcome_balanced.csv for analysis or reporting.\")\n"
   ],
   "id": "f8a7323360ddae3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (21545, 2) instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 60\u001B[0m\n\u001B[0;32m     57\u001B[0m     y_pred_proba_balanced \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict_proba(X_test_balanced)\n\u001B[0;32m     59\u001B[0m class_report_balanced \u001B[38;5;241m=\u001B[39m classification_report(y_test_balanced, y_pred_balanced)\n\u001B[1;32m---> 60\u001B[0m roc_auc_balanced \u001B[38;5;241m=\u001B[39m \u001B[43mroc_auc_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_test_balanced\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred_proba_balanced\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmulti_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43movr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maverage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweighted\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m evaluated. Saving results...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     63\u001B[0m f\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mClassification Report for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mclass_report_balanced\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\Projects\\HealthIntel\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    212\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    213\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    214\u001B[0m         )\n\u001B[0;32m    215\u001B[0m     ):\n\u001B[1;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    221\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    222\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    223\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    224\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    225\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    226\u001B[0m     )\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\Projects\\HealthIntel\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:641\u001B[0m, in \u001B[0;36mroc_auc_score\u001B[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001B[0m\n\u001B[0;32m    639\u001B[0m     labels \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(y_true)\n\u001B[0;32m    640\u001B[0m     y_true \u001B[38;5;241m=\u001B[39m label_binarize(y_true, classes\u001B[38;5;241m=\u001B[39mlabels)[:, \u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m--> 641\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_average_binary_score\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    642\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpartial\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_binary_roc_auc_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_fpr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_fpr\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    643\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    644\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    645\u001B[0m \u001B[43m        \u001B[49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    646\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    647\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    648\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# multilabel-indicator\u001B[39;00m\n\u001B[0;32m    649\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _average_binary_score(\n\u001B[0;32m    650\u001B[0m         partial(_binary_roc_auc_score, max_fpr\u001B[38;5;241m=\u001B[39mmax_fpr),\n\u001B[0;32m    651\u001B[0m         y_true,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    654\u001B[0m         sample_weight\u001B[38;5;241m=\u001B[39msample_weight,\n\u001B[0;32m    655\u001B[0m     )\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\Projects\\HealthIntel\\.venv\\lib\\site-packages\\sklearn\\metrics\\_base.py:69\u001B[0m, in \u001B[0;36m_average_binary_score\u001B[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001B[0m\n\u001B[0;32m     66\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m format is not supported\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(y_type))\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 69\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbinary_metric\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     71\u001B[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001B[0;32m     72\u001B[0m y_true \u001B[38;5;241m=\u001B[39m check_array(y_true)\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\Projects\\HealthIntel\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:388\u001B[0m, in \u001B[0;36m_binary_roc_auc_score\u001B[1;34m(y_true, y_score, sample_weight, max_fpr)\u001B[0m\n\u001B[0;32m    379\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    380\u001B[0m         (\n\u001B[0;32m    381\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOnly one class is present in y_true. ROC AUC score \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    384\u001B[0m         UndefinedMetricWarning,\n\u001B[0;32m    385\u001B[0m     )\n\u001B[0;32m    386\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mnan\n\u001B[1;32m--> 388\u001B[0m fpr, tpr, _ \u001B[38;5;241m=\u001B[39m \u001B[43mroc_curve\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    389\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m max_fpr \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m max_fpr \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    390\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m auc(fpr, tpr)\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\Projects\\HealthIntel\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    187\u001B[0m global_skip_validation \u001B[38;5;241m=\u001B[39m get_config()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip_parameter_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[1;32m--> 189\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    191\u001B[0m func_sig \u001B[38;5;241m=\u001B[39m signature(func)\n\u001B[0;32m    193\u001B[0m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\Projects\\HealthIntel\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:1150\u001B[0m, in \u001B[0;36mroc_curve\u001B[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001B[0m\n\u001B[0;32m   1046\u001B[0m \u001B[38;5;129m@validate_params\u001B[39m(\n\u001B[0;32m   1047\u001B[0m     {\n\u001B[0;32m   1048\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray-like\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1057\u001B[0m     y_true, y_score, \u001B[38;5;241m*\u001B[39m, pos_label\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, drop_intermediate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1058\u001B[0m ):\n\u001B[0;32m   1059\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001B[39;00m\n\u001B[0;32m   1060\u001B[0m \n\u001B[0;32m   1061\u001B[0m \u001B[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1148\u001B[0m \u001B[38;5;124;03m    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\u001B[39;00m\n\u001B[0;32m   1149\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1150\u001B[0m     fps, tps, thresholds \u001B[38;5;241m=\u001B[39m \u001B[43m_binary_clf_curve\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1151\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpos_label\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\n\u001B[0;32m   1152\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1154\u001B[0m     \u001B[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001B[39;00m\n\u001B[0;32m   1155\u001B[0m     \u001B[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001B[39;00m\n\u001B[0;32m   1156\u001B[0m     \u001B[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1161\u001B[0m     \u001B[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001B[39;00m\n\u001B[0;32m   1162\u001B[0m     \u001B[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001B[39;00m\n\u001B[0;32m   1163\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m drop_intermediate \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(fps) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m:\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\Projects\\HealthIntel\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:822\u001B[0m, in \u001B[0;36m_binary_clf_curve\u001B[1;34m(y_true, y_score, pos_label, sample_weight)\u001B[0m\n\u001B[0;32m    820\u001B[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001B[0;32m    821\u001B[0m y_true \u001B[38;5;241m=\u001B[39m column_or_1d(y_true)\n\u001B[1;32m--> 822\u001B[0m y_score \u001B[38;5;241m=\u001B[39m \u001B[43mcolumn_or_1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_score\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    823\u001B[0m assert_all_finite(y_true)\n\u001B[0;32m    824\u001B[0m assert_all_finite(y_score)\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\Projects\\HealthIntel\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:1485\u001B[0m, in \u001B[0;36mcolumn_or_1d\u001B[1;34m(y, dtype, warn, device)\u001B[0m\n\u001B[0;32m   1472\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1473\u001B[0m             (\n\u001B[0;32m   1474\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA column-vector y was passed when a 1d array was\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1479\u001B[0m             stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m   1480\u001B[0m         )\n\u001B[0;32m   1481\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _asarray_with_order(\n\u001B[0;32m   1482\u001B[0m         xp\u001B[38;5;241m.\u001B[39mreshape(y, (\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,)), order\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC\u001B[39m\u001B[38;5;124m\"\u001B[39m, xp\u001B[38;5;241m=\u001B[39mxp, device\u001B[38;5;241m=\u001B[39mdevice\n\u001B[0;32m   1483\u001B[0m     )\n\u001B[1;32m-> 1485\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1486\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my should be a 1d array, got an array of shape \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(shape)\n\u001B[0;32m   1487\u001B[0m )\n",
      "\u001B[1;31mValueError\u001B[0m: y should be a 1d array, got an array of shape (21545, 2) instead."
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
