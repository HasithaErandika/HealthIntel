{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-02T13:56:30.996054Z",
     "start_time": "2025-08-02T13:56:04.840554Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from joblib import load\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths\n",
    "processed_data_path = '../data/processed/'\n",
    "models_path = '../models/'\n",
    "\n",
    "# Load preprocessed data\n",
    "X_test = np.load(os.path.join(processed_data_path, 'X_test.npy'))\n",
    "y_test = np.load(os.path.join(processed_data_path, 'y_test.npy'))\n",
    "\n",
    "# Load models\n",
    "log_reg = load(os.path.join(models_path, 'logistic_regression_smote.pkl'))\n",
    "rf_model = load(os.path.join(models_path, 'random_forest_smote_tuned.pkl'))\n",
    "nn_model = load_model(os.path.join(models_path, 'neural_net_tuned.h5'))\n",
    "\n",
    "# Function to evaluate model with classification report and ROC-AUC\n",
    "def evaluate_model(model, model_name, X_test, y_test):\n",
    "    if model_name == 'Neural Network':\n",
    "        y_pred_proba = model.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "    # Classification Report\n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # ROC-AUC Score\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "    print(f\"ROC-AUC Score for {model_name}: {roc_auc:.4f}\")\n",
    "\n",
    "# Evaluate all models\n",
    "models = {'Logistic Regression': log_reg, 'Random Forest': rf_model, 'Neural Network': nn_model}\n",
    "for name, model in models.items():\n",
    "    evaluate_model(model, name, X_test, y_test)\n",
    "\n",
    "# Save results to file\n",
    "with open(os.path.join(models_path, 'evaluation_results.txt'), 'w') as f:\n",
    "    for name, model in models.items():\n",
    "        if name == 'Neural Network':\n",
    "            y_pred_proba = model.predict(X_test)\n",
    "            y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test)\n",
    "        f.write(f\"\\nClassification Report for {name}:\\n\")\n",
    "        f.write(classification_report(y_test, y_pred))\n",
    "        f.write(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted'):.4f}\\n\")\n",
    "\n",
    "print(\"\\nEvaluation complete. Results saved to evaluation_results.txt in models/ directory.\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.63      0.75     65029\n",
      "         1.0       0.04      0.30      0.07      1974\n",
      "         2.0       0.33      0.57      0.42     11474\n",
      "\n",
      "    accuracy                           0.61     78477\n",
      "   macro avg       0.44      0.50      0.41     78477\n",
      "weighted avg       0.83      0.61      0.69     78477\n",
      "\n",
      "ROC-AUC Score for Logistic Regression: 0.7886\n",
      "\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.95      0.89     65029\n",
      "         1.0       0.03      0.01      0.01      1974\n",
      "         2.0       0.36      0.15      0.21     11474\n",
      "\n",
      "    accuracy                           0.81     78477\n",
      "   macro avg       0.41      0.37      0.37     78477\n",
      "weighted avg       0.75      0.81      0.77     78477\n",
      "\n",
      "ROC-AUC Score for Random Forest: 0.7437\n",
      "\u001B[1m2453/2453\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 798us/step\n",
      "\n",
      "Classification Report for Neural Network:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.75      0.83     65029\n",
      "         1.0       0.11      0.00      0.00      1974\n",
      "         2.0       0.32      0.71      0.44     11474\n",
      "\n",
      "    accuracy                           0.72     78477\n",
      "   macro avg       0.45      0.49      0.42     78477\n",
      "weighted avg       0.81      0.72      0.75     78477\n",
      "\n",
      "ROC-AUC Score for Neural Network: 0.7907\n",
      "\u001B[1m2453/2453\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 647us/step\n",
      "\n",
      "Evaluation complete. Results saved to evaluation_results.txt in models/ directory.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T14:01:17.852802Z",
     "start_time": "2025-08-02T14:01:14.858928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths\n",
    "processed_data_path = '../data/processed/'\n",
    "models_path = '../models/'\n",
    "\n",
    "# Load preprocessed data\n",
    "X_test = np.load(os.path.join(processed_data_path, 'X_test.npy'))\n",
    "y_test = np.load(os.path.join(processed_data_path, 'y_test.npy'))\n",
    "\n",
    "# Load the Neural Network model (best model based on ROC-AUC)\n",
    "nn_model = load_model(os.path.join(models_path, 'neural_net_tuned.h5'))\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_proba = nn_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Map predictions to labels (assuming 0.0: No Diabetes, 1.0: Yes no complications, 2.0: Yes with complications)\n",
    "label_map = {0: 'No Diabetes', 1: 'Yes, no complications', 2: 'Yes, with complications'}\n",
    "y_pred_labels = [label_map[pred] for pred in y_pred]\n",
    "y_test_labels = [label_map[true] for true in y_test]\n",
    "\n",
    "# Create a summary DataFrame with predictions and confidence scores\n",
    "results_df = pd.DataFrame({\n",
    "    'True Label': y_test_labels,\n",
    "    'Predicted Label': y_pred_labels,\n",
    "    'Confidence Score': [y_pred_proba[i, pred] for i, pred in enumerate(y_pred)]\n",
    "})\n",
    "\n",
    "# Display the first 10 rows as a quick outcome\n",
    "print(\"\\nQuick Outcome Summary (First 10 Predictions):\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "# Save the full results to a CSV file for further use\n",
    "results_df.to_csv(os.path.join(models_path, 'predictions_outcome.csv'), index=False)\n",
    "print(\"\\nFull prediction results saved to predictions_outcome.csv in models/ directory.\")\n",
    "\n",
    "# Optional: Basic performance summary\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "print(\"\\nPerformance Summary for Neural Network:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted'):.4f}\")\n",
    "\n",
    "print(\"\\nOutcome generated. You can use predictions_outcome.csv for analysis or reporting. Retrain later to improve 1.0 recall if needed.\")"
   ],
   "id": "f8a7323360ddae3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m2453/2453\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 660us/step\n",
      "\n",
      "Quick Outcome Summary (First 10 Predictions):\n",
      "                True Label          Predicted Label  Confidence Score\n",
      "0  Yes, with complications  Yes, with complications          0.486102\n",
      "1              No Diabetes              No Diabetes          0.516496\n",
      "2              No Diabetes  Yes, with complications          0.549559\n",
      "3              No Diabetes              No Diabetes          0.745143\n",
      "4              No Diabetes              No Diabetes          0.546921\n",
      "5              No Diabetes  Yes, with complications          0.619402\n",
      "6  Yes, with complications  Yes, with complications          0.389203\n",
      "7              No Diabetes              No Diabetes          0.444533\n",
      "8  Yes, with complications  Yes, with complications          0.787049\n",
      "9              No Diabetes              No Diabetes          0.411554\n",
      "\n",
      "Full prediction results saved to predictions_outcome.csv in models/ directory.\n",
      "\n",
      "Performance Summary for Neural Network:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.75      0.83     65029\n",
      "         1.0       0.11      0.00      0.00      1974\n",
      "         2.0       0.32      0.71      0.44     11474\n",
      "\n",
      "    accuracy                           0.72     78477\n",
      "   macro avg       0.45      0.49      0.42     78477\n",
      "weighted avg       0.81      0.72      0.75     78477\n",
      "\n",
      "ROC-AUC Score: 0.7907\n",
      "\n",
      "Outcome generated. You can use predictions_outcome.csv for analysis or reporting. Retrain later to improve 1.0 recall if needed.\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
